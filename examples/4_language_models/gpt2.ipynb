{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2382c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d55bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 small (decoder-only Transformer)\n",
    "model_name = \"gpt2\"  # ~124M parameters\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b403ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5257e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt\n",
    "prompt = \"ChatGPT explains things\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5f2e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b420c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set generation parameters\n",
    "max_new_tokens = 10\n",
    "pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "temperature = 0.8       # Lower = more deterministic, Higher = more random\n",
    "top_k = 50              # Sample only from top 50 logits\n",
    "top_p = 0.9             # Nucleus sampling: sample from smallest set of tokens whose cumulative prob >= 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd2fcc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tokens autoregressively with pad_token_id explicitly set\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=torch.ones_like(input_ids),\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=True,         # Must enable sampling to use top-k/top-p/temperature\n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    "    temperature=temperature,\n",
    "    pad_token_id=pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cbb97b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: ChatGPT explains things\n",
      "Generated text: ChatGPT explains things about the program.\n",
      "\n",
      "In this episode we\n"
     ]
    }
   ],
   "source": [
    "# Decode to text\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c769a603",
   "metadata": {},
   "source": [
    "example with step-by-step generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6da864e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt tokens: [30820, 38, 11571, 6688, 1243]\n"
     ]
    }
   ],
   "source": [
    "# Initialize sequence\n",
    "generated_ids = input_ids.clone()\n",
    "\n",
    "print(\"Initial prompt tokens:\", generated_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02693f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: ['Chat', 'G', 'PT', 'Ġexplains', 'Ġthings']\n"
     ]
    }
   ],
   "source": [
    "print(\"Prompt tokens:\", tokenizer.convert_ids_to_tokens(input_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "747c82b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: sequence length = 5\n",
      "Causal attention mask:\n",
      "tensor([[0, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "Step 1: Generated token ID=345, word=' you', prob=0.0195\n",
      "\n",
      "Step 2: sequence length = 6\n",
      "Causal attention mask:\n",
      "tensor([[0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "Step 2: Generated token ID=1244, word=' might', prob=0.0660\n",
      "\n",
      "Step 3: sequence length = 7\n",
      "Causal attention mask:\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "Step 3: Generated token ID=407, word=' not', prob=0.7946\n",
      "\n",
      "Step 4: sequence length = 8\n",
      "Causal attention mask:\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "Step 4: Generated token ID=760, word=' know', prob=0.6185\n",
      "\n",
      "Step 5: sequence length = 9\n",
      "Causal attention mask:\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "Step 5: Generated token ID=13, word='.', prob=0.3792\n",
      "\n",
      "Step 6: sequence length = 10\n",
      "Causal attention mask:\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "Step 6: Generated token ID=198, word='\n",
      "', prob=0.7717\n",
      "\n",
      "Step 7: sequence length = 11\n",
      "Causal attention mask:\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "Step 7: Generated token ID=198, word='\n",
      "', prob=1.0000\n",
      "\n",
      "Step 8: sequence length = 12\n",
      "Causal attention mask:\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "Step 8: Generated token ID=5962, word='First', prob=0.0031\n",
      "\n",
      "Step 9: sequence length = 13\n",
      "Causal attention mask:\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "Step 9: Generated token ID=11, word=',', prob=0.6094\n",
      "\n",
      "Step 10: sequence length = 14\n",
      "Causal attention mask:\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "Step 10: Generated token ID=612, word=' there', prob=0.0803\n",
      "\n",
      "Full generated text:\n",
      " ChatGPT explains things you might not know.\n",
      "\n",
      "First, there\n"
     ]
    }
   ],
   "source": [
    "# Autoregressive generation step-by-step\n",
    "for step in range(max_new_tokens):\n",
    "    # Generate causal mask (upper triangular)\n",
    "    seq_len = generated_ids.size(1)\n",
    "    causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    \n",
    "    print(f\"\\nStep {step+1}: sequence length = {seq_len}\")\n",
    "    print(\"Causal attention mask:\")\n",
    "    print(causal_mask.int())  # 1 = masked, 0 = attendable\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(generated_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Get logits for the last token\n",
    "    next_token_logits = logits[0, -1, :]\n",
    "\n",
    "    # Apply temperature\n",
    "    next_token_logits = next_token_logits / temperature\n",
    "\n",
    "    # Compute probabilities\n",
    "    probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "    # Sample next token\n",
    "    next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "    generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0)], dim=1)\n",
    "\n",
    "    # Decode last token to word\n",
    "    next_token_word = tokenizer.decode(next_token_id)\n",
    "    next_token_prob = probs[next_token_id].item()\n",
    "    print(f\"Step {step+1}: Generated token ID={next_token_id.item()}, word='{next_token_word}', prob={next_token_prob:.4f}\")\n",
    "\n",
    "# Decode full generated text\n",
    "full_generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"\\nFull generated text:\\n\", full_generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
