{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data taken from https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    dev = \"mps\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_data1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
    "cifar_data1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = cifar_data1[b'data']\n",
    "labels = cifar_data1[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_batch in [\"cifar-10-batches-py/data_batch_2\",\n",
    "                   \"cifar-10-batches-py/data_batch_3\",\n",
    "                   \"cifar-10-batches-py/data_batch_4\",\n",
    "                   \"cifar-10-batches-py/data_batch_5\"]:\n",
    "    data_dict = unpickle(data_batch)\n",
    "\n",
    "    images = np.append(images, data_dict[b'data'], axis=0)\n",
    "    labels = labels + data_dict[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = unpickle(\"cifar-10-batches-py/test_batch\")\n",
    "test_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_data[b'data']\n",
    "test_labels = test_data[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert to one-hot targets for multi-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.]], device='mps:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor = torch.nn.functional.one_hot(torch.tensor(labels)).to(dtype=torch.float32).to(device)\n",
    "test_target_tensor = torch.tensor(test_labels)\n",
    "target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.tensor(images).to(dtype=torch.float32).to(device)\n",
    "test_tensor = torch.tensor(test_images).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ds = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "test_ds = torch.utils.data.TensorDataset(test_tensor, test_target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = torch.utils.data.random_split(input_ds, [0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 64\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=mini_batch_size, shuffle=True, drop_last=False)\n",
    "valid_dl = torch.utils.data.DataLoader(val_ds, batch_size=mini_batch_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fcn = nn.Sequential(\n",
    "            nn.Linear(3072, 3000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3000, 2000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2000, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.fcn(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of trainable model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17773610"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        model.load_state_dict(self.best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=5, delta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, optimizer, train_dl, valid_dl=None):\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        # loop over mini-batches\n",
    "        for X_mb, y_mb in train_dl:\n",
    "            y_hat = model(X_mb)\n",
    "\n",
    "            loss = loss_func(y_hat, y_mb)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_loss = sum(loss_func(model(X_mb), y_mb) for X_mb, y_mb in train_dl)\n",
    "            valid_loss = sum(loss_func(model(X_mb), y_mb) for X_mb, y_mb in valid_dl)\n",
    "        print('epoch {}, training loss {}'.format(epoch + 1, train_loss / len(train_dl)))\n",
    "        print('epoch {}, validation loss {}'.format(epoch + 1, valid_loss / len(valid_dl)))\n",
    "\n",
    "        early_stopping(valid_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    print('Finished training')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, training loss 1.8320415019989014\n",
      "epoch 1, validation loss 1.8176853656768799\n",
      "epoch 2, training loss 1.735809326171875\n",
      "epoch 2, validation loss 1.722028374671936\n",
      "epoch 3, training loss 1.7056403160095215\n",
      "epoch 3, validation loss 1.7233415842056274\n",
      "epoch 4, training loss 1.6360268592834473\n",
      "epoch 4, validation loss 1.6406323909759521\n",
      "epoch 5, training loss 1.6145004034042358\n",
      "epoch 5, validation loss 1.6317249536514282\n",
      "epoch 6, training loss 1.58390474319458\n",
      "epoch 6, validation loss 1.6007716655731201\n",
      "epoch 7, training loss 1.5879112482070923\n",
      "epoch 7, validation loss 1.6154510974884033\n",
      "epoch 8, training loss 1.6189764738082886\n",
      "epoch 8, validation loss 1.6524364948272705\n",
      "epoch 9, training loss 1.569131851196289\n",
      "epoch 9, validation loss 1.611169457435608\n",
      "epoch 10, training loss 1.4945003986358643\n",
      "epoch 10, validation loss 1.5475890636444092\n",
      "epoch 11, training loss 1.5218746662139893\n",
      "epoch 11, validation loss 1.5678530931472778\n",
      "epoch 12, training loss 1.5570906400680542\n",
      "epoch 12, validation loss 1.6149749755859375\n",
      "epoch 13, training loss 1.5094143152236938\n",
      "epoch 13, validation loss 1.5732675790786743\n",
      "epoch 14, training loss 1.4644032716751099\n",
      "epoch 14, validation loss 1.5491788387298584\n",
      "epoch 15, training loss 1.448328971862793\n",
      "epoch 15, validation loss 1.540995717048645\n",
      "epoch 16, training loss 1.432560682296753\n",
      "epoch 16, validation loss 1.5502359867095947\n",
      "epoch 17, training loss 1.4280482530593872\n",
      "epoch 17, validation loss 1.5395758152008057\n",
      "epoch 18, training loss 1.4597651958465576\n",
      "epoch 18, validation loss 1.5727537870407104\n",
      "epoch 19, training loss 1.4647629261016846\n",
      "epoch 19, validation loss 1.5871860980987549\n",
      "epoch 20, training loss 1.411300539970398\n",
      "epoch 20, validation loss 1.5440140962600708\n",
      "epoch 21, training loss 1.3913577795028687\n",
      "epoch 21, validation loss 1.5323474407196045\n",
      "epoch 22, training loss 1.4366990327835083\n",
      "epoch 22, validation loss 1.594516634941101\n",
      "epoch 23, training loss 1.400235891342163\n",
      "epoch 23, validation loss 1.5747138261795044\n",
      "epoch 24, training loss 1.3618232011795044\n",
      "epoch 24, validation loss 1.551236867904663\n",
      "epoch 25, training loss 1.3825606107711792\n",
      "epoch 25, validation loss 1.5747945308685303\n",
      "epoch 26, training loss 1.3959954977035522\n",
      "epoch 26, validation loss 1.5934182405471802\n",
      "Early stopping\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "trained_model = fit(epochs, model, optimizer, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping.load_best_model(model)\n",
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(ds, model, train=False):\n",
    "    with torch.no_grad():\n",
    "        preds = model(ds[:][0].cpu())\n",
    "\n",
    "    # take output node with highest probability\n",
    "    yhat = np.argmax(preds, axis=1)\n",
    "    y = ds[:][1].cpu()\n",
    "\n",
    "    # from one-hot back to labels\n",
    "    if train:\n",
    "        y = torch.argmax(y, dim=1)\n",
    "\n",
    "    metric = MulticlassAccuracy()\n",
    "\n",
    "    metric.update(yhat, y)\n",
    "    return metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4956)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(train_ds, trained_model, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4464)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(val_ds, model, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4420)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(test_ds, trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy with batch1 training data only: 40%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
